{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vri/Projects/research/Flood-detection\n"
     ]
    }
   ],
   "source": [
    "%cd '/home/vri/Projects/research/Flood-detection'\n",
    "\n",
    "# necessary for cross directory imports. \n",
    "# replace path with root dir of projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import numpy as np \n",
    "\n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "\n",
    "import rasterio\n",
    "import cv2\n",
    "from rasterio.plot import show\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn , utils, Tensor\n",
    "from torchvision import transforms as T\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.optim as optim\n",
    "\n",
    "#module for preprocessing data directory structure\n",
    "from utils import utils_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/home/vri/Projects/research/Flood-detection/data/transformer_data/sen12floods_s2_source'\n",
    "LABEL_PATH = './data/transformer_labels/sen12floods_s2_labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of valid image subfolders are: 1949\n"
     ]
    }
   ],
   "source": [
    "#list of valid data paths\n",
    "actual_flist = utils_dirs.get_img_folders(flag='valid', root_path = ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_labels(dirname, label_dir):\n",
    "    \"\"\"function to get image label for corresponding image dir\n",
    "\n",
    "    Args:\n",
    "        dirname (str): name of image subdirectory\n",
    "        label_dir (path.path): path to label directory\n",
    "    \"\"\"\n",
    "    pd = dirname.split(\"_\")\n",
    "    pd = f\"{pd[0]}_{pd[1]}_labels_{pd[3]}_{pd[4]}_{pd[5]}_{pd[6]}\"\n",
    "\n",
    "    json_data = open (f\"{label_dir}/{pd}/stac.json\", \"rb\")\n",
    "    jdata = json.load(json_data)\n",
    "    flood = jdata[\"properties\"][\"FLOODING\"]\n",
    "\n",
    "    image_label = 1 if flood else 0\n",
    "\n",
    "    return image_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"pytorch custom dataset class. \n",
    "    Inherits from torch.utils.data.Dataset\n",
    "\n",
    "    Args:\n",
    "        img_list (list): list of valid image paths (flist)\n",
    "        label_path (pathlib.path) : path to labels dir\n",
    "        transform (torchvision.transforms): image transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_list, label_path, transform = None ):\n",
    "        self.transform = transform\n",
    "        self.img_list = img_list\n",
    "        self.label_path = label_path\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = utils_dirs.get_dirname(self.img_list[idx])\n",
    "        label = get_image_labels(img_name, self.label_path)\n",
    "\n",
    "        img = cv2.imread(f\"{self.img_list[idx]}/stack.tif\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MEAN, STD=(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = T.Compose([\n",
    "    T.ToTensor() ,\n",
    "    T.RandomHorizontalFlip(p = 0.5) ,\n",
    "    T.RandomVerticalFlip (p = 0.5) ,\n",
    "    T.Resize((224, 224), antialias=True),\n",
    "    T.Normalize(MEAN, STD)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in train set: 1325\n",
      "Images in test set: 292\n",
      "Images in val set: 332\n"
     ]
    }
   ],
   "source": [
    "total_dataset = CustomDataset(actual_flist, LABEL_PATH, data_transforms)\n",
    "\n",
    "test_size = int(0.15 * len(total_dataset))\n",
    "\n",
    "train_size = int(0.8*(len(total_dataset) - test_size))\n",
    "val_size = len(total_dataset)- test_size - train_size\n",
    "\n",
    "train_set , test_set, val_set = random_split(total_dataset, [train_size, test_size, val_size])\n",
    "\n",
    "print(f\"Images in train set: {len(train_set)}\")\n",
    "print(f\"Images in test set: {len(test_set)}\")\n",
    "print(f\"Images in val set: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, num_workers=16)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Resnet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet18(nn.Module):\n",
    "    \"\"\"Custom Resnet-18 class.\n",
    "    Inherits from torch.nn.Module\n",
    "    Args:\n",
    "        num_classes (int): number of classes\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "        self.model = models.resnet18(weights = models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBinaryClassifier(pl.LightningModule):\n",
    "    \"\"\" Pytorch lightning module for Resnet Binary Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes:int, config = None):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet_model = CustomResNet18(num_classes= 2)\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, img_batch):\n",
    "        return self.resnet_model(img_batch)\n",
    "    \n",
    "    def common_step(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self.forward (x)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        correct = logits.argmax(dim=1).eq(y).sum().item()\n",
    "        accuracy = self.accuracy(correct, y)\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    def accuracy(self, correct, labels):\n",
    "        return correct / len(labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch)\n",
    "        \n",
    "        self.log_dict({\n",
    "            \"train_loss\": loss,\n",
    "            \"train_accuracy\" : accuracy,\n",
    "        }, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_accuracy', accuracy, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        correct = logits.argmax(dim=1).eq(y).sum().items()\n",
    "        total = len(y)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        self.log('test_accuracy', accuracy, prog_bar=True)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr = self.config['lr'])\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lr' : 1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(config, num_epochs = 10, checkpoint = None):\n",
    "    \"\"\"function to fit pytorch lightning trainer\n",
    "\n",
    "    Args:\n",
    "        config (dict): configuration values\n",
    "        num_epochs (int): Number of epochs. Defaults to 10.\n",
    "        checkpoint (_type_): model checkpoint if training from a saved point. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        trainer: pl.trainer\n",
    "    \"\"\"\n",
    "    model = ResNetBinaryClassifier(2, config)\n",
    "    tlogger = TensorBoardLogger(save_dir=\"flood-detection-logs\", name=\"resnet-model\", version = \"v1\")\n",
    "\n",
    "    resnet_callbacks = [\n",
    "        # early_stopping,\n",
    "        callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1,\n",
    "                                  save_on_train_epoch_end= False,\n",
    "                                  filename='{epoch}-{val_loss:.2f}')\n",
    "    ]\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(accelerator= \"gpu\",\n",
    "                         logger= tlogger,\n",
    "                         log_every_n_steps= 2,\n",
    "                         precision=16,\n",
    "                         enable_checkpointing= True,\n",
    "                         callbacks= resnet_callbacks ,\n",
    "                         devices= 1,\n",
    "                         enable_progress_bar= True,\n",
    "                         max_epochs= num_epochs\n",
    "                         )\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders= val_loader, ckpt_path = checkpoint)\n",
    "    return trainer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/raster_env/lib/python3.11/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type           | Params\n",
      "------------------------------------------------\n",
      "0 | resnet_model | CustomResNet18 | 11.2 M\n",
      "------------------------------------------------\n",
      "1.0 K     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 42/42 [00:08<00:00,  4.70it/s, v_num=v1, train_loss=0.313, train_accuracy=1.000, val_loss=0.313, val_accuracy=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 42/42 [00:08<00:00,  4.70it/s, v_num=v1, train_loss=0.313, train_accuracy=1.000, val_loss=0.313, val_accuracy=1.000]\n"
     ]
    }
   ],
   "source": [
    "resnet_trainer = train_resnet(config= config, num_epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at flood-detection-logs/resnet-model/v1/checkpoints/epoch=9-val_loss=0.31.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at flood-detection-logs/resnet-model/v1/checkpoints/epoch=9-val_loss=0.31.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 33.80it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Runningstage.validating metric      DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy                  1.0\n",
      "        val_loss            0.3133895993232727\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.3133895993232727, 'val_accuracy': 1.0}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_trainer.validate(dataloaders=test_loader, ckpt_path='best')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTModule(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ViTModule, self).__init__()\n",
    "        self.model=models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTClassifier(pl.LightningModule):\n",
    "    def __init__(self,config = None):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.resnet_model = ViTModule()\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, img_batch):\n",
    "        return self.resnet_model(img_batch)\n",
    "    \n",
    "    def common_step(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self.forward (x)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        correct = logits.argmax(dim=1).eq(y).sum().item()\n",
    "        accuracy = self.accuracy(correct, y)\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    def accuracy(self, correct, labels):\n",
    "        return correct / len(labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch)\n",
    "        \n",
    "        self.log_dict({\n",
    "            \"train_loss\": loss,\n",
    "            \"train_accuracy\" : accuracy,\n",
    "        }, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_accuracy', accuracy, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        correct = logits.argmax(dim=1).eq(y).sum().items()\n",
    "        total = len(y)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        self.log('test_accuracy', accuracy, prog_bar=True)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr = self.config['lr'])\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_config ={\n",
    "    'lr': 1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vit(config, num_epochs = 10, checkpoint = None):\n",
    "    model = ViTClassifier( config)\n",
    "    tlogger = TensorBoardLogger(save_dir=\"flood-detection-logs\", name=\"vit-model\", version = \"v1\")\n",
    "\n",
    "    vit_callbacks = [\n",
    "        # early_stopping,\n",
    "        callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1,\n",
    "                                  save_on_train_epoch_end= False,\n",
    "                                  filename='{epoch}-{val_loss:.2f}')\n",
    "    ]\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(accelerator= \"gpu\",\n",
    "                         logger= tlogger,\n",
    "                         log_every_n_steps= 2,\n",
    "                         precision=16,\n",
    "                         enable_checkpointing= True,\n",
    "                         callbacks= vit_callbacks ,\n",
    "                         devices= 1,\n",
    "                         enable_progress_bar= True,\n",
    "                         max_epochs= num_epochs\n",
    "                         )\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders= val_loader, ckpt_path = checkpoint)\n",
    "    return trainer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vri/miniconda3/envs/raster_env/lib/python3.11/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type      | Params\n",
      "-------------------------------------------\n",
      "0 | resnet_model | ViTModule | 86.6 M\n",
      "-------------------------------------------\n",
      "86.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "86.6 M    Total params\n",
      "346.271   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 42/42 [00:18<00:00,  2.28it/s, v_num=v1, train_loss=7.46e-6, train_accuracy=1.000, val_loss=7.44e-6, val_accuracy=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 42/42 [00:18<00:00,  2.28it/s, v_num=v1, train_loss=7.46e-6, train_accuracy=1.000, val_loss=7.44e-6, val_accuracy=1.000]\n"
     ]
    }
   ],
   "source": [
    "vit_trainer = train_vit(config= vit_config, num_epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at flood-detection-logs/vit-model/v1/checkpoints/epoch=9-val_loss=0.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at flood-detection-logs/vit-model/v1/checkpoints/epoch=9-val_loss=0.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 11.90it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Runningstage.validating metric      DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy                  1.0\n",
      "        val_loss           7.442823971359758e-06\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 7.442823971359758e-06, 'val_accuracy': 1.0}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_trainer.validate(dataloaders=test_loader, ckpt_path='best')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SWIN Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinModule(nn.Module):\n",
    "    \"\"\"class for tiny Swin transformer v2\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SwinModule, self).__init__()\n",
    "        self.model=models.swin_v2_t(weights=models.Swin_V2_T_Weights.DEFAULT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWINClassifier(pl.LightningModule):\n",
    "    def __init__(self,config = None):\n",
    "        super(SWINClassifier, self).__init__()\n",
    "        self.swin_model = SwinModule()\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, img_batch):\n",
    "        return self.swin_model(img_batch)\n",
    "    \n",
    "    def common_step(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self.forward (x)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        correct = logits.argmax(dim=1).eq(y).sum().item()\n",
    "        accuracy = self.accuracy(correct, y)\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    def accuracy(self, correct, labels):\n",
    "        return correct / len(labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch)\n",
    "        \n",
    "        self.log_dict({\n",
    "            \"train_loss\": loss,\n",
    "            \"train_accuracy\" : accuracy,\n",
    "        }, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_accuracy', accuracy, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        correct = logits.argmax(dim=1).eq(y).sum().items()\n",
    "        total = len(y)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        self.log('test_accuracy', accuracy, prog_bar=True)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr = self.config['lr'])\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_config ={\n",
    "    'lr': 1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_swin(config, num_epochs = 10, checkpoint = None):\n",
    "    model = SWINClassifier( config)\n",
    "    tlogger = TensorBoardLogger(save_dir=\"flood-detection-logs\", name=\"swin-model\", version = \"v1\")\n",
    "\n",
    "    vit_callbacks = [\n",
    "        # early_stopping,\n",
    "        callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1,\n",
    "                                  save_on_train_epoch_end= False,\n",
    "                                  filename='{epoch}-{val_loss:.2f}')\n",
    "    ]\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(accelerator= \"gpu\",\n",
    "                         logger= tlogger,\n",
    "                         log_every_n_steps= 2,\n",
    "                         precision=16,\n",
    "                         enable_checkpointing= True,\n",
    "                         callbacks= vit_callbacks ,\n",
    "                         devices= 1,\n",
    "                         enable_progress_bar= True,\n",
    "                         max_epochs= num_epochs\n",
    "                         )\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders= val_loader, ckpt_path = checkpoint)\n",
    "    return trainer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | swin_model | SwinModule | 28.4 M\n",
      "------------------------------------------\n",
      "28.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.4 M    Total params\n",
      "113.406   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 42/42 [00:16<00:00,  2.47it/s, v_num=v1, train_loss=7.98e-7, train_accuracy=1.000, val_loss=0.000, val_accuracy=1.000]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 42/42 [00:16<00:00,  2.47it/s, v_num=v1, train_loss=7.98e-7, train_accuracy=1.000, val_loss=0.000, val_accuracy=1.000]\n"
     ]
    }
   ],
   "source": [
    "swin_trainer = train_swin(config= swin_config, num_epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at flood-detection-logs/swin-model/v1/checkpoints/epoch=3-val_loss=0.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at flood-detection-logs/swin-model/v1/checkpoints/epoch=3-val_loss=0.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 12.58it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Runningstage.validating metric      DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy                  1.0\n",
      "        val_loss                    0.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.0, 'val_accuracy': 1.0}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_trainer.validate(dataloaders=test_loader, ckpt_path='best')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trial Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e509747faab13124\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e509747faab13124\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8081;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualise metric plots via tensorboard\n",
    "#ensure tensorflow is installed on local device and path to logdir is accurate\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=\"/home/vri/Projects/research/Flood-detection/flood-detection-logs\" --host localhost --port 8081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raster_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
